{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPtSVYr/+0jos2B74PXyzzh",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaviabey/nlpresearch/blob/main/preprocessing.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dependency Installation  "
      ],
      "metadata": {
        "id": "EXyqQ6Sh2NzT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OYZlkVY61XqF",
        "outputId": "7e3d493d-c221-43cb-b41f-17a9ae60857c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.1.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2023.12.25)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install   -U spacy\n",
        "!pip  install  nltk"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "spaCY installation\n"
      ],
      "metadata": {
        "id": "B3Wly_GL2fhJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python -m spacy download en_core_web_sm"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aTdjFODC2vGC",
        "outputId": "d1b72ecd-d0e6-495c-c098-9c9e8d3e716f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting en-core-web-sm==3.7.1\n",
            "  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.1/en_core_web_sm-3.7.1-py3-none-any.whl (12.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.8/12.8 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: spacy<3.8.0,>=3.7.2 in /usr/local/lib/python3.10/dist-packages (from en-core-web-sm==3.7.1) (3.7.4)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.2.3)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.2)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.4.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.3.4)\n",
            "Requirement already satisfied: typer<0.10.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.9.4)\n",
            "Requirement already satisfied: smart-open<7.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (6.4.0)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.66.2)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.31.0)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.7.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.1.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (67.7.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (24.0)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.25.2)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.6.0)\n",
            "Requirement already satisfied: pydantic-core==2.18.2 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.18.2)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2024.2.2)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.1.4)\n",
            "Requirement already satisfied: click<9.0.0,>=7.1.1 in /usr/local/lib/python3.10/dist-packages (from typer<0.10.0,>=0.3.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (8.1.7)\n",
            "Requirement already satisfied: cloudpathlib<0.17.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.4.0,>=0.1.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (0.16.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy<3.8.0,>=3.7.2->en-core-web-sm==3.7.1) (1.1.0)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the package via spacy.load('en_core_web_sm')\n",
            "\u001b[38;5;3m⚠ Restart to reload dependencies\u001b[0m\n",
            "If you are in a Jupyter or Colab notebook, you may need to restart Python in\n",
            "order to load all the package's dependencies. You can do this by selecting the\n",
            "'Restart kernel' or 'Restart runtime' option.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy,nltk\n",
        "nltk.download('punkt')"
      ],
      "metadata": {
        "id": "7thAG7193JY4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77659e09-d882-40ad-b8e3-60824c6adacf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Tokenization"
      ],
      "metadata": {
        "id": "-OyE_6ky3NP3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlp   =  spacy.load('en_core_web_sm')\n",
        "nlp\n",
        "S = \"he paid $6000 for his degree\"\n",
        "S.split(\" \")\n",
        "print([ t.text for t in nlp(S)])"
      ],
      "metadata": {
        "id": "73xSsq3o3Pdd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2d91bc3e-1f08-49f9-8387-7d7a9f16b2e7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['he', 'paid', '$', '6000', 'for', 'his', 'degree']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "s = \"he paid  600$  in colombo one gall face\"\n",
        "s.split(\" \")"
      ],
      "metadata": {
        "id": "cGq3RZlI30lJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "doc = nlp(\"I have one sister . Apple is looking at buying U.K . startup for $1 billion. Here and there she saw maps and pictures hung upon pegs.Nut it was too dark to see anything.Then she looked at  the sides of the well\")\n",
        "print(doc)\n",
        "for sent in doc.sents:\n",
        "    print(sent)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L99lCDBF78e8",
        "outputId": "b283a416-dcb6-446d-c408-cb200c675e85"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "I have one sister . Apple is looking at buying U.K . startup for $1 billion. Here and there she saw maps and pictures hung upon pegs.Nut it was too dark to see anything.Then she looked at  the sides of the well\n",
            "I have one sister .\n",
            "Apple is looking at buying U.K .\n",
            "startup for $1 billion.\n",
            "Here and there she saw maps and pictures hung upon pegs.\n",
            "Nut it was too dark to see anything.\n",
            "Then she looked at  the sides of the well\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "s = \"he paid 6000$   for his degree\"\n",
        "s = \"Tokenization is the process of breaking a piece of text into smaller units, such as words or sentences. In NLTK, you can use the word_tokenize() function to tokenize a piece of text into words. Here is an example:\"\n",
        "nltk.tokenize.sent_tokenize(s)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KtTKRf9V7_cZ",
        "outputId": "473b7c10-88d5-4980-c7c5-f2d1aac11c0c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Tokenization is the process of breaking a piece of text into smaller units, such as words or sentences.',\n",
              " 'In NLTK, you can use the word_tokenize() function to tokenize a piece of text into words.',\n",
              " 'Here is an example:']"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Text Preprocessing\n"
      ],
      "metadata": {
        "id": "Gf6jW_zVHf9c"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import spacy\n",
        "\n",
        "nlp = spacy.load('en_core_web_sm')\n",
        "\n",
        "text = \"There is a pen on the table\"\n",
        "\n",
        "doc = nlp(text)\n",
        "\n",
        "filtered_words = [t.text for t in doc if not t.is_stop]\n",
        "\n",
        "clean_text =  ' '.join(filtered_words)\n"
      ],
      "metadata": {
        "id": "knIvM0EsHnbc"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stop_words_spacyy =   nlp.Defaults.stop_words\n",
        "print(stop_words_spacyy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zPu2m8F-OBXT",
        "outputId": "592703ad-e9e7-4676-ebfd-d0f180ce05a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'seem', 'each', 'to', 'around', 'empty', 'everywhere', '‘re', 'why', 'herein', 'whoever', 'but', 'we', 'otherwise', 'else', 'few', 'her', '’m', 'in', 'per', 'throughout', 'wherein', 'then', 'therein', 'become', 'anything', 'fifty', 'under', 'take', 'ours', 'less', 'thereafter', '‘ll', 'afterwards', 'among', 'sometime', 'out', \"'d\", 'she', 'whither', 'many', 'cannot', 'would', 'there', 'behind', 'no', 'them', '‘m', 'former', 'always', 'while', 'doing', 'amount', 'unless', 'wherever', 'sixty', 'whenever', 'onto', 'off', 'whereas', 'not', 'whereby', 'mine', 'their', 'latterly', 'becoming', 'did', 'give', 'latter', 'though', 'so', 'least', \"'ll\", 'up', 'meanwhile', 'whether', 'although', 'i', 'from', 'that', 'everyone', 'myself', 'get', 'any', 'how', '‘d', 'first', 'one', 'already', 'hereupon', 'three', 'also', 'too', 'seems', 'several', 'us', 'besides', 'must', 'which', 'own', 'used', '’ll', 'the', 'just', 'noone', 'therefore', 'hereafter', 'for', 'make', 'quite', 'when', \"'s\", 'call', 'has', 'being', 'everything', 'anyhow', 'becomes', 'both', 'within', 'made', 'more', 'often', 'those', 'others', 'even', 'where', \"'ve\", 'alone', 'after', '’s', 'without', 'between', '‘ve', 'nine', 'very', 'into', 'whereafter', 'third', 'through', 'anywhere', 'became', 'various', '‘s', 'themselves', 'again', 'or', 'thereupon', 'somehow', 'namely', 're', 'most', 'about', 'perhaps', 'seeming', 'beyond', 'hence', 'two', 'thru', 'might', 'anyway', 'twenty', 'top', 'should', 'nobody', 'these', 'however', 'show', 'fifteen', 'thereby', 'rather', 'at', 'ten', 'nothing', 'was', 'across', 'it', 'yet', 'hers', 'if', 'they', 'well', 'n‘t', 'of', 'such', 'only', 'here', 'my', 'me', 'by', 'last', 'toward', 'forty', 'he', '’d', 'put', 'what', 'may', 'an', 'really', 'other', 'you', 'anyone', 'go', 'move', 'whom', \"n't\", 'together', 'with', 'does', 'eleven', 'since', 'been', 'below', 'another', 'whose', 'via', 'could', '’re', 'along', 'whole', 'can', 'done', 'please', 'keep', 'regarding', 'himself', 'on', 'sometimes', 'herself', 'hereby', 'because', 'whatever', 'never', 'have', 'indeed', 'four', 'bottom', 'hundred', 'down', \"'re\", 'all', 'beforehand', 'this', 'full', 'upon', 'serious', 'its', 'a', 'either', 'elsewhere', 'now', 'mostly', 'over', 'during', 'part', 'neither', 'front', \"'m\", 'much', 'name', 'same', 'nevertheless', 'yours', 'eight', 'who', 'are', 'beside', 'whereupon', 'ourselves', 'is', 'before', 'further', 'nor', 'somewhere', 'him', 'due', 'once', 'next', 'enough', 'above', 'some', '’ve', 'against', 'his', 'nowhere', 'something', 'our', 'none', 'formerly', 'ever', 'n’t', 'back', 'yourself', 'and', 'be', 'towards', 'except', 'ca', 'seemed', 'had', 'five', 'than', 'twelve', 'say', 'using', 'itself', 'almost', 'whence', 'your', 'moreover', 'see', 'were', 'will', 'thus', 'someone', 'thence', 'yourselves', 'side', 'amongst', 'until', 'as', 'do', 'six', 'am', 'every', 'still'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Stemming\n"
      ],
      "metadata": {
        "id": "a5DQhEE9EuhD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.stem.snowball import  SnowballStemmer\n",
        "\n",
        "s_ = [t.text for t in doc if t not in stop_words_spacyy  ]\n",
        "snow_stemmer =  SnowballStemmer('english')\n",
        "s__ = []\n",
        "print(s_)\n",
        "for t in s_ :\n",
        "   s =  snow_stemmer.stem(t)\n",
        "   s__.append(s)\n",
        "\n",
        "\n",
        "\n",
        "print(s__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5UBolO_JEw6a",
        "outputId": "0359f9e2-c2c1-4d46-f2e9-cd74d1d8b390"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['There', 'is', 'a', 'pen', 'on', 'the', 'table']\n",
            "['there', 'is', 'a', 'pen', 'on', 'the', 'tabl']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Lemmatization as a solution to stemming problem  \n"
      ],
      "metadata": {
        "id": "wioYaQYNIvLZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "nltk.download('wordnet')\n",
        "lemmetizer  =  WordNetLemmatizer()\n",
        "s = [t.text for t in doc  if t.text.lower() not in stop_words_spacyy]\n",
        "s__ = []\n",
        "\n",
        "for  t in s :\n",
        "   x =  lemmetizer.lemmatize(t)\n",
        "   s__.append(x)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zCWK4PBLIt3e",
        "outputId": "f30befe1-0166-4148-f20a-690c6001bfd8"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "So preprocessing involved tokeniation . stop-word removal , case folding  , lemmatization and stemming\n"
      ],
      "metadata": {
        "id": "YjbyfFLBLACD"
      }
    }
  ]
}